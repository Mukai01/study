# 1. Hadoop
* SparkはHadoopが持っている問題に対処するために出てきた
* DatabricksはSparkが持っている問題に対処するために出てきた
* Hadoop
    * 非効率な処理で時間がかかる
    * Batchのみのサポート
    * Huge ecosystem
* Spark : データプロセスからMachine Learningまで可能
    * Hadoopよりも早い
    * Python, RのAPIを提供
    * Real Time processingもサポート
    * 実行が早い
    * Master nodeとWorker nodeからなる
        * Workder NodeにExecutorが含まれている
        * Worker Nodeはストレージを持たず、キャッシュする
    * しかし使うのが難しい
    * 環境を自分で作らないといけない
    * 他の人とのCollaborationに適していない
    * Cloud用の開発ではなくデプロイが簡単ではない
* databricks
    * Sparkで動くためのプラットフォーム
    * SparkをmanageするためのWorkspaceを提供する
    * Collaborationが簡単
    * Architecture
        * Workspace
            * Run time : Azure ML SDK, mlflow等
                * Cloud Services : Azure, AWS等

# 2. DataBricksの準備
* DataBricksWorkspaceを作成する
    * DataBricks ⇒ 作成
* Clusters ⇒ Create Cluster
    * Cluster Name : mydbcluster001
    * Cluster Mode : Standard
    * Pool : None (standbyしておいて、起動時間を減らすオプション)
    * Databricks Runtime : 8.2 Scala 2.12
    * Enable autoscaling : MinとMaxを決めるが今回はOff
    * Terminate after 120 minutes of inactivity
    * Worker Type : Standard_DS3_v2, Workers : 1
    * Driver Type : Same as worker

# 3. Azure ML Workspaceと接続
* Azure Databricks ⇒ 概要 ⇒ Link Azure ML
    * Workspace type : Use Existing として選択する

# 4. Notebook
* Library ⇒ Install new ⇒ PYPI
    * azureml-sdk[databricks] とする
* Create ⇒ Notebook 
    * Name : test001
* Workspace ⇒ Import ⇒ dbcファイルをインポートするとNotebookをインポートできる
    