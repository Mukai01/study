{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db65919",
   "metadata": {},
   "source": [
    "# 画像のタグ抽出\n",
    "* Computer Visionリソースが必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf1a1684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-vision-computervision in c:\\users\\nakam\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: msrest>=0.5.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-computervision) (0.7.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-computervision) (1.1.28)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2020.12.5)\n",
      "Requirement already satisfied: requests~=2.16 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.28.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (0.6.1)\n",
      "Requirement already satisfied: azure-core>=1.24.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.25.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-cognitiveservices-vision-computervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5171a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Tag an image - remote =====\n",
      "result⇒ [<azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9850>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9700>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9820>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9190>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9910>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9970>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF99D0>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9A30>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9A90>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9AF0>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9B50>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9BB0>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9C10>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9C70>, <azure.cognitiveservices.vision.computervision.models._models_py3.ImageTag object at 0x0000027D27EF9CD0>]\n",
      "Tags in the remote image: \n",
      "\n",
      "'outdoor' with confidence 99.00%\n",
      "'building' with confidence 98.81%\n",
      "'sky' with confidence 98.21%\n",
      "'stadium' with confidence 98.17%\n",
      "'ancient rome' with confidence 96.16%\n",
      "'ruins' with confidence 95.04%\n",
      "'amphitheatre' with confidence 93.99%\n",
      "'ancient roman architecture' with confidence 92.65%\n",
      "'historic site' with confidence 89.55%\n",
      "'ancient history' with confidence 89.54%\n",
      "'history' with confidence 86.72%\n",
      "'archaeological site' with confidence 84.41%\n",
      "'travel' with confidence 65.85%\n",
      "'large' with confidence 61.02%\n",
      "'city' with confidence 56.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "from array import array\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "\n",
    "subscription_key = \"\"\n",
    "endpoint = \"https://computervisionai102.cognitiveservices.azure.com/\"\n",
    "\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "remote_image_url = \"https://raw.githubusercontent.com/gottagetgit/AI102Files/main/Computer_Vision/Analyze_images_using_Computer_Vision_API/Images/Landmark.jpg \"\n",
    "\n",
    "'''\n",
    "Tag an Image - remote\n",
    "This example returns a tag (key word) for each thing in the image.\n",
    "'''\n",
    "print(\"===== Tag an image - remote =====\")\n",
    "# Call API with remote image\n",
    "tags_result_remote = computervision_client.tag_image(remote_image_url)\n",
    "print('result⇒', tags_result_remote.tags)\n",
    "\n",
    "# Print results with confidence score\n",
    "print(\"Tags in the remote image: \\n\")\n",
    "if len(tags_result_remote.tags) == 0:\n",
    "    print(\"No tags detected.\")\n",
    "else:\n",
    "    for tag in tags_result_remote.tags:\n",
    "        print(\"'{}' with confidence {:.2f}%\".format(tag.name, tag.confidence * 100))\n",
    "print()\n",
    "\n",
    "\n",
    "# ローカルファイル用プログラム\n",
    "# print(\"===== Tag an Image - local =====\")\n",
    "# local_image_path = \"Images/Landmark.jpg\"\n",
    "# local_image = open(local_image_path, \"rb\")\n",
    "# # Call API local image\n",
    "# tags_result_local = computervision_client.tag_image_in_stream(local_image)\n",
    "\n",
    "# # Print results with confidence score\n",
    "# print(\"Tags in the local image: \\n\")\n",
    "# if len(tags_result_local.tags) == 0:\n",
    "#     print(\"No tags detected.\")\n",
    "# else:\n",
    "#     for tag in tags_result_local.tags:\n",
    "#         print(\"'{}' with confidence {:.2f}%\".format(tag.name, tag.confidence * 100))\n",
    "# print()\n",
    "# '''\n",
    "# END - Tag an Image - local\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ca5e9",
   "metadata": {},
   "source": [
    "# 画像のDescription抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6151999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Describe an image - remote =====\n",
      "result⇒ [<azure.cognitiveservices.vision.computervision.models._models_py3.ImageCaption object at 0x0000027D278E34F0>]\n",
      "Description of remote image: \n",
      "'an ancient city with many ruins with Colosseum in the background' with confidence 33.80%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Describe an Image - remote\n",
    "This example describes the contents of an image with the confidence score.\n",
    "'''\n",
    "print(\"===== Describe an image - remote =====\")\n",
    "# describe_imageを使う\n",
    "description_results = computervision_client.describe_image(remote_image_url)\n",
    "\n",
    "print('result⇒', description_results.captions)\n",
    "print(\"Description of remote image: \")\n",
    "if len(description_results.captions) == 0:\n",
    "    print(\"No description detected.\")\n",
    "else:\n",
    "    for caption in description_results.captions:\n",
    "        print(\"'{}' with confidence {:.2f}%\".format(caption.text, caption.confidence * 100))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de19028",
   "metadata": {},
   "source": [
    "# Landmark検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa02405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Detect Domain-specific Content - Remote =====\n",
      "result⇒ {'additional_properties': {}, 'result': {'landmarks': [{'name': 'Colosseum', 'confidence': 0.9369766712188721}]}, 'request_id': '03c69947-f041-4522-ab63-7d846b61738c', 'metadata': <azure.cognitiveservices.vision.computervision.models._models_py3.ImageMetadata object at 0x0000027D2789DBE0>, 'model_version': '2021-04-01'}\n",
      "Landmarks in the remote image:\n",
      "Colosseum\n"
     ]
    }
   ],
   "source": [
    "# landmarkに強いモデルを使うのでコロッセオを認識できる\n",
    "# \"celebrity\"を使うと有名人に強いモデルになる\n",
    "detect_domain_results_landmarks = computervision_client.analyze_image_by_domain(\"landmarks\", remote_image_url)\n",
    "print(\"===== Detect Domain-specific Content - Remote =====\")\n",
    "print('result⇒', detect_domain_results_landmarks)\n",
    "print(\"Landmarks in the remote image:\")\n",
    "if len(detect_domain_results_landmarks.result[\"landmarks\"]) == 0:\n",
    "    print(\"No landmarks detected.\")\n",
    "else:\n",
    "    for landmark in detect_domain_results_landmarks.result[\"landmarks\"]:\n",
    "        print(landmark[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298190bf",
   "metadata": {},
   "source": [
    "# ブランド検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9c9844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Detect Brands - remote =====\n",
      "result⇒ {'additional_properties': {}, 'categories': None, 'adult': None, 'color': None, 'image_type': None, 'tags': None, 'description': None, 'faces': None, 'objects': None, 'brands': [<azure.cognitiveservices.vision.computervision.models._models_py3.DetectedBrand object at 0x0000027D27F37D90>, <azure.cognitiveservices.vision.computervision.models._models_py3.DetectedBrand object at 0x0000027D27F37EE0>], 'request_id': '834689ee-8c97-41d8-8ba1-4cb6eaeb83ad', 'metadata': <azure.cognitiveservices.vision.computervision.models._models_py3.ImageMetadata object at 0x0000027D27F37BE0>, 'model_version': '2021-05-01'}\n",
      "Detecting brands in remote image: \n",
      "\n",
      "'Microsoft' brand detected with confidence 62.5% at location 58, 113, 106, 152\n",
      "'Microsoft' brand detected with confidence 69.8% at location 58, 260, 86, 149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Detect Brands - remote =====\")\n",
    "# Get a URL with a brand logo\n",
    "remote_image_url = \"https://raw.githubusercontent.com/gottagetgit/AI102Files/main/Computer_Vision/Analyze_images_using_Computer_Vision_API/Images/gray-shirt-logo.jpg\"\n",
    "# Select the visual feature(s) you want\n",
    "remote_image_features = [\"brands\"]\n",
    "# Call API with URL and features\n",
    "detect_brands_results_remote = computervision_client.analyze_image(remote_image_url, remote_image_features)\n",
    "print('result⇒', detect_brands_results_remote)\n",
    "\n",
    "print(\"Detecting brands in remote image: \\n\")\n",
    "if len(detect_brands_results_remote.brands) == 0:\n",
    "    print(\"No brands detected.\")\n",
    "else:\n",
    "    for brand in detect_brands_results_remote.brands:\n",
    "        print(\"'{}' brand detected with confidence {:.1f}% at location {}, {}, {}, {}\".format(\n",
    "            brand.name, brand.confidence * 100, brand.rectangle.x, brand.rectangle.x + brand.rectangle.w,\n",
    "            brand.rectangle.y, brand.rectangle.y + brand.rectangle.h))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651179e",
   "metadata": {},
   "source": [
    "# Moderate Adult Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86848604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Detect Adult or Racy Content - remote =====\n",
      "result⇒ {'additional_properties': {}, 'categories': None, 'adult': <azure.cognitiveservices.vision.computervision.models._models_py3.AdultInfo object at 0x0000027D27F36220>, 'color': None, 'image_type': None, 'tags': None, 'description': None, 'faces': None, 'objects': None, 'brands': None, 'request_id': '397de5d1-3f3a-4a55-8bbb-1cc9b88a1819', 'metadata': <azure.cognitiveservices.vision.computervision.models._models_py3.ImageMetadata object at 0x0000027D27F36250>, 'model_version': '2021-05-01'}\n",
      "Analyzing remote image for adult or racy content ... \n",
      "Is adult content: True with confidence 99.80\n",
      "Has racy content: False with confidence 99.88\n"
     ]
    }
   ],
   "source": [
    "remote_image_url = \"https://blog-imgs-145.fc2.com/h/n/a/hnalady/nude27_1.jpg\"\n",
    "\n",
    "print(\"===== Detect Adult or Racy Content - remote =====\")\n",
    "# Select the visual feature(s) you want\n",
    "remote_image_features = [\"adult\"]\n",
    "# Call API with URL and features\n",
    "detect_adult_results_remote = computervision_client.analyze_image(remote_image_url, remote_image_features)\n",
    "print('result⇒', detect_adult_results_remote)\n",
    "\n",
    "# Print results with adult/racy score\n",
    "print(\"Analyzing remote image for adult or racy content ... \")\n",
    "print(\"Is adult content: {} with confidence {:.2f}\".format(detect_adult_results_remote.adult.is_adult_content, detect_adult_results_remote.adult.adult_score * 100))\n",
    "print(\"Has racy content: {} with confidence {:.2f}\".format(detect_adult_results_remote.adult.is_racy_content, detect_adult_results_remote.adult.racy_score * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a7278",
   "metadata": {},
   "source": [
    "# サムネイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd30516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thumbnail saved to local folder.\n",
      "\n",
      "===== Generate Thumbnails - remote =====\n",
      "Generating thumbnail from a URL image...\n",
      "Thumbnail saved to local folder.\n"
     ]
    }
   ],
   "source": [
    "# もし空白が多い写真であれば、focalポイントを切り出す\n",
    "print(\"Thumbnail saved to local folder.\")\n",
    "print()\n",
    "print(\"===== Generate Thumbnails - remote =====\")\n",
    "# Generate a thumbnail from a URL image\n",
    "# URL of faces\n",
    "remote_image_url_thumb = \"https://raw.githubusercontent.com/gottagetgit/AI102Files/main/Computer_Vision/Analyze_images_using_Computer_Vision_API/Images/Faces.jpg \"\n",
    "\n",
    "print(\"Generating thumbnail from a URL image...\")\n",
    "# 100, 100は画像の大きさ、Trueはsmart Cropping\n",
    "thumb_remote = computervision_client.generate_thumbnail(\n",
    "    100, 100, remote_image_url_thumb, True)\n",
    "\n",
    "# binaryファイルが返ってくる\n",
    "with open(\"thumb_remote.png\", \"wb\") as f:\n",
    "    for chunk in thumb_remote:\n",
    "        f.write(chunk)\n",
    "\n",
    "print(\"Thumbnail saved to local folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec513d7",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb4f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Detect Printed Text with OCR - remote =====\n",
      "\n",
      "Bounding box: 79,94,1109,81\n",
      "Nutrition Facts Amount Per Serving \n",
      "Bounding box: 220,171,334,51\n",
      "see: bar (40g) \n",
      "Bounding box: 654,205,321,50\n",
      "Total Fat 13g \n",
      "Bounding box: 45,230,473,56\n",
      "Servng Per Package: 4 \n",
      "Bounding box: 682,287,433,50\n",
      "Saturated t 1.5 g \n",
      "Bounding box: 11,347,474,50\n",
      "Amount Per Serving \n",
      "Bounding box: 19,436,256,43\n",
      "alories 190 \n",
      "Bounding box: 32,517,397,43\n",
      "ories from Fat 110 \n",
      "Bounding box: 72,618,394,41\n",
      "t Daily Values are based \n",
      "Bounding box: 673,362,289,53\n",
      "Trans Fat Og \n",
      "Bounding box: 613,438,422,60\n",
      "Cholesterol Omg \n",
      "Bounding box: 598,517,358,53\n",
      "Sodium 20mq \n"
     ]
    }
   ],
   "source": [
    "print(\"===== Detect Printed Text with OCR - remote =====\")\n",
    "print()\n",
    "remote_printed_text_image_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/printed_text.jpg \"\n",
    "\n",
    "ocr_result_remote = computervision_client.recognize_printed_text(remote_printed_text_image_url)\n",
    "for region in ocr_result_remote.regions:\n",
    "    for line in region.lines:\n",
    "        print(\"Bounding box: {}\".format(line.bounding_box))\n",
    "        s = \"\"\n",
    "        for word in line.words:\n",
    "            s += word.text + \" \"\n",
    "        print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50dee38",
   "metadata": {},
   "source": [
    "# 手書き文字の検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06353c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Detect Printed Text with the Read API - remote =====\n",
      "\n",
      "The quick brown fox jumps\n",
      "[87.0, 653.0, 2574.0, 712.0, 2571.0, 865.0, 83.0, 820.0]\n",
      "over the lazy dog!\n",
      "[177.0, 1008.0, 1988.0, 1010.0, 1987.0, 1159.0, 177.0, 1150.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Detect Printed Text with the Read API - remote =====\")\n",
    "print()\n",
    "# Get an image with handwritten text\n",
    "remote_image_handw_text_url = \"https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/cognitive-services/Computer-vision/Images/readsample.jpg \"\n",
    "\n",
    "# Call API with URL and raw response (allows you to get the operation location)\n",
    "recognize_handw_results = computervision_client.read(remote_image_handw_text_url,  raw=True)\n",
    "\n",
    "# Get the operation location (URL with an ID at the end) from the response\n",
    "operation_location_remote = recognize_handw_results.headers[\"Operation-Location\"]\n",
    "# Grab the ID from the URL\n",
    "operation_id = operation_location_remote.split(\"/\")[-1]\n",
    "\n",
    "# Call the \"GET\" API and wait for it to retrieve the results\n",
    "# 処理に時間がかかるため、待つ処理を入れている\n",
    "while True:\n",
    "    get_handw_text_results = computervision_client.get_read_result(operation_id)\n",
    "    if get_handw_text_results.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the detected text, line by line\n",
    "if get_handw_text_results.status == OperationStatusCodes.succeeded:\n",
    "    for text_result in get_handw_text_results.analyze_result.read_results:\n",
    "        for line in text_result.lines:\n",
    "            print(line.text)\n",
    "            print(line.bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b2b72",
   "metadata": {},
   "source": [
    "# Form Recognizer\n",
    "* Form Recognizerリソースが必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55e44fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-formrecognizer==3.2.0b6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (3.2.0b6)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-ai-formrecognizer==3.2.0b6) (1.25.1)\n",
      "Requirement already satisfied: msrest>=0.6.21 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-ai-formrecognizer==3.2.0b6) (0.7.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-ai-formrecognizer==3.2.0b6) (1.1.28)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-ai-formrecognizer==3.2.0b6) (4.3.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer==3.2.0b6) (2.28.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer==3.2.0b6) (1.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b6) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b6) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b6) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer==3.2.0b6) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer==3.2.0b6) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer==3.2.0b6) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b6) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-formrecognizer==3.2.0b6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25c49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Recognizing form--------\n",
      "Table found on page 1:\n",
      "Cell text: SUBTOTAL\n",
      "Location: [Point(x=1070.0, y=1564.0), Point(x=1307.0, y=1564.0), Point(x=1308.0, y=1609.0), Point(x=1071.0, y=1608.0)]\n",
      "Confidence score: 1.0\n",
      "\n",
      "Cell text: $140.00\n",
      "Location: [Point(x=1307.0, y=1564.0), Point(x=1544.0, y=1564.0), Point(x=1544.0, y=1609.0), Point(x=1308.0, y=1609.0)]\n",
      "Confidence score: 1.0\n",
      "\n",
      "Cell text: TAX\n",
      "Location: [Point(x=1071.0, y=1608.0), Point(x=1308.0, y=1609.0), Point(x=1308.0, y=1652.0), Point(x=1071.0, y=1653.0)]\n",
      "Confidence score: 1.0\n",
      "\n",
      "Cell text: $4.00\n",
      "Location: [Point(x=1308.0, y=1609.0), Point(x=1544.0, y=1609.0), Point(x=1544.0, y=1652.0), Point(x=1308.0, y=1652.0)]\n",
      "Confidence score: 1.0\n",
      "\n",
      "Cell text: \n",
      "Location: [Point(x=1071.0, y=1653.0), Point(x=1308.0, y=1652.0), Point(x=1308.0, y=1664.0), Point(x=1071.0, y=1664.0)]\n",
      "Confidence score: 1.0\n",
      "\n",
      "Cell text: \n",
      "Location: [Point(x=1308.0, y=1652.0), Point(x=1544.0, y=1652.0), Point(x=1544.0, y=1665.0), Point(x=1308.0, y=1664.0)]\n",
      "Confidence score: 1.0\n",
      "\n",
      "Cell text: TOTAL\n",
      "Location: [Point(x=1071.0, y=1664.0), Point(x=1308.0, y=1664.0), Point(x=1308.0, y=1707.0), Point(x=1071.0, y=1707.0)]\n",
      "Confidence score: 1.0\n",
      "\n",
      "Cell text: $144.00\n",
      "Location: [Point(x=1308.0, y=1664.0), Point(x=1544.0, y=1665.0), Point(x=1544.0, y=1707.0), Point(x=1308.0, y=1707.0)]\n",
      "Confidence score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.ai.formrecognizer import FormRecognizerClient\n",
    "from azure.ai.formrecognizer import FormTrainingClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "key = \"\"\n",
    "endpoint = \"https://formrecognizer0701.cognitiveservices.azure.com/\"\n",
    "\n",
    "form_recognizer_client = FormRecognizerClient(endpoint, AzureKeyCredential(key))\n",
    "form_training_client = FormTrainingClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "formUrl = \"https://raw.githubusercontent.com/Azure/azure-sdk-for-python/master/sdk/formrecognizer/azure-ai-formrecognizer/tests/sample_forms/forms/Form_1.jpg \"\n",
    "\n",
    "print(\"--------Recognizing form--------\")\n",
    "\n",
    "poller = form_recognizer_client.begin_recognize_content_from_url(formUrl)\n",
    "page = poller.result()\n",
    "\n",
    "# 表が2つ検出される\n",
    "table = page[0].tables[1]  # page 1, table 1\n",
    "print(\"Table found on page {}:\".format(table.page_number))\n",
    "for cell in table.cells:\n",
    "    print(\"Cell text: {}\".format(cell.text))\n",
    "    print(\"Location: {}\".format(cell.bounding_box))\n",
    "    print(\"Confidence score: {}\\n\".format(cell.confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b494a115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Recognizing receipt--------\n",
      "[RecognizedForm(form_type=prebuilt:receipt, fields={'Items': FormField(value_type=list, label_data=None, value_data=None, name=Items, value=[FormField(value_type=dictionary, label_data=None, value_data=None, name=Items, value={'Name': FormField(value_type=string, label_data=None, value_data=FieldData(page_number=1, text=Surface Pro 6, bounding_box=[Point(x=364.0, y=1559.0), Point(x=675.0, y=1561.7), Point(x=674.5, y=1625.7), Point(x=363.5, y=1623.0)], field_elements=None), name=Name, value='Surface Pro 6', confidence=0.914), 'Quantity': FormField(value_type=float, label_data=None, value_data=FieldData(page_number=1, text=1, bounding_box=[Point(x=327.0, y=1558.0), Point(x=352.0, y=1559.0), Point(x=351.0, y=1623.0), Point(x=327.0, y=1623.0)], field_elements=None), name=Quantity, value=1.0, confidence=0.971), 'TotalPrice': FormField(value_type=float, label_data=None, value_data=FieldData(page_number=1, text=999.00, bounding_box=[Point(x=974.0, y=1792.0), Point(x=1135.0, y=1796.0), Point(x=1133.0, y=1859.0), Point]\n",
      "\n",
      "Receipt Items:\n",
      "\n",
      "...Item #1\n",
      "......Name: Surface Pro 6 has confidence 0.914\n",
      "......Quantity: 1.0 has confidence 0.971\n",
      "......TotalPrice: 999.0 has confidence 0.983\n",
      "\n",
      "...Item #2\n",
      "......Name: SurfacePen has confidence 0.718\n",
      "......Quantity: 1.0 has confidence 0.976\n",
      "......TotalPrice: 99.99 has confidence 0.967\n",
      "\n",
      "MerchantAddress: 123 Main Street Redmond, WA 98052 has confidence 0.975\n",
      "\n",
      "MerchantName: Contoso has confidence 0.974\n",
      "\n",
      "MerchantPhoneNumber: None has confidence 0.988\n",
      "\n",
      "ReceiptType: Itemized has confidence 0.99\n",
      "\n",
      "Subtotal: 1098.99 has confidence 0.982\n",
      "\n",
      "Tax: 104.4 has confidence 0.985\n",
      "\n",
      "Total: 1203.39 has confidence 0.957\n",
      "\n",
      "TransactionDate: 2019-06-10 has confidence 0.987\n",
      "\n",
      "TransactionTime: 13:59:00 has confidence 0.985\n"
     ]
    }
   ],
   "source": [
    "# レシート検出\n",
    "receiptUrl = \"https://raw.githubusercontent.com/Azure/azure-sdk-for-python/master/sdk/formrecognizer/azure-ai-formrecognizer/tests/sample_forms/receipt/contoso-receipt.png \"\n",
    "\n",
    "print(\"--------Recognizing receipt--------\")\n",
    "poller = form_recognizer_client.begin_recognize_receipts_from_url(receiptUrl)\n",
    "result = poller.result()\n",
    "print(result)\n",
    "\n",
    "for receipt in result:\n",
    "    for name, field in receipt.fields.items():\n",
    "        # 商品情報\n",
    "        if name == \"Items\":\n",
    "            print()\n",
    "            print(\"Receipt Items:\")\n",
    "            for idx, items in enumerate(field.value):\n",
    "                print()\n",
    "                print(\"...Item #{}\".format(idx + 1))\n",
    "                for item_name, item in items.value.items():\n",
    "                    print(\"......{}: {} has confidence {}\".format(item_name, item.value, item.confidence))\n",
    "        # 商品以外の情報\n",
    "        else:\n",
    "            print()\n",
    "            print(\"{}: {} has confidence {}\".format(name, field.value, field.confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df5718",
   "metadata": {},
   "source": [
    "# 顔検出\n",
    "* Face APIリソースが必要\n",
    "* 承認が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c321b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-vision-face in c:\\users\\nakam\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: msrest>=0.5.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-face) (0.7.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-face) (1.1.28)\n",
      "Requirement already satisfied: requests~=2.16 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.28.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.3.0)\n",
      "Requirement already satisfied: azure-core>=1.24.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2020.12.5)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (0.6.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade azure-cognitiveservices-vision-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9474670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import io\n",
    "# import glob\n",
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# import uuid\n",
    "# import requests\n",
    "# from urllib.parse import urlparse\n",
    "# from io import BytesIO\n",
    "\n",
    "# from PIL import Image, ImageDraw\n",
    "# from azure.cognitiveservices.vision.face import FaceClient\n",
    "# from msrest.authentication import CognitiveServicesCredentials\n",
    "# from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person\n",
    "\n",
    "# subscription_key = \"\"\n",
    "# endpoint = \"\"\n",
    "\n",
    "# # Create an authenticated FaceClient.\n",
    "# face_client = FaceClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "# # Detect a face in an image that contains a single face\n",
    "# single_face_image_url = 'https://www.biography.com/.image/t_share/MTQ1MzAyNzYzOTgxNTE0NTEz/john-f-kennedy---mini-biography.jpg '\n",
    "# single_image_name = os.path.basename(single_face_image_url)\n",
    "# # We use detection model 3 to get better performance.\n",
    "# detected_faces = face_client.face.detect_with_url(url=single_face_image_url, detection_model='detection_03')\n",
    "# if not detected_faces:\n",
    "#     raise Exception('No face detected from image {}'.format(single_image_name))\n",
    "\n",
    "# # Display the detected face ID in the first single-face image.\n",
    "# # Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
    "# print(\"==== Detect faces in an image ===\")\n",
    "# print()\n",
    "# print('Detected face ID from', single_image_name, ':')\n",
    "# for face in detected_faces: print(face.face_id)\n",
    "\n",
    "# # Save this ID for use in Find Similar\n",
    "# first_image_face_ID = detected_faces[0].face_id\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38fe9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 似た顔を見つける\n",
    "# print(\"===== Detect the faces in an image that contains multiple faces =====\")\n",
    "# print()\n",
    "# # Detect the faces in an image that contains multiple faces\n",
    "# # Each detected face gets assigned a new ID\n",
    "# multi_face_image_url = \"http://www.historyplace.com/kennedy/president-family-portrait-closeup.jpg\"\n",
    "# multi_image_name = os.path.basename(multi_face_image_url)\n",
    "# # We use detection model 3 to get better performance.\n",
    "# detected_faces2 = face_client.face.detect_with_url(url=multi_face_image_url, detection_model='detection_03')\n",
    "\n",
    "# # Search through faces detected in group image for the single face from first image.\n",
    "# # First, create a list of the face IDs found in the second image.\n",
    "# second_image_face_IDs = list(map(lambda x: x.face_id, detected_faces2))\n",
    "# # Next, find similar face IDs like the one detected in the first image.\n",
    "# similar_faces = face_client.face.find_similar(face_id=first_image_face_ID, face_ids=second_image_face_IDs)\n",
    "# if not similar_faces:\n",
    "#     print('No similar faces found in', multi_image_name, '.')\n",
    "\n",
    "# # Print the details of the similar faces detected\n",
    "# else:\n",
    "#     print('Similar faces found in', multi_image_name + ':')\n",
    "#     for face in similar_faces:\n",
    "#         first_image_face_ID = face.face_id\n",
    "#         # The similar face IDs of the single face image and the group image do not need to match,\n",
    "#         # they are only used for identification purposes in each image.\n",
    "#         # The similar faces are matched using the Cognitive Services algorithm in find_similar().\n",
    "#         face_info = next(x for x in detected_faces2 if x.face_id == first_image_face_ID)\n",
    "#         if face_info:\n",
    "#             print('  Face ID: ', first_image_face_ID)\n",
    "#             print('  Face rectangle:')\n",
    "#             print('    Left: ', str(face_info.face_rectangle.left))\n",
    "#             print('    Top: ', str(face_info.face_rectangle.top))\n",
    "#             print('    Width: ', str(face_info.face_rectangle.width))\n",
    "#             print('    Height: ', str(face_info.face_rectangle.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30521073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 画像を学習させて、どの人かを検出させる\n",
    "\n",
    "# import asyncio\n",
    "# import io\n",
    "# import glob\n",
    "# import os\n",
    "# import sys\n",
    "# import time\n",
    "# import uuid\n",
    "# import requests\n",
    "# from urllib.parse import urlparse\n",
    "# from io import BytesIO\n",
    "# from PIL import Image, ImageDraw\n",
    "# from azure.cognitiveservices.vision.face import FaceClient\n",
    "# from msrest.authentication import CognitiveServicesCredentials\n",
    "# from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person\n",
    "\n",
    "# # This key will serve all examples in this document.\n",
    "# KEY = \"\"\n",
    "\n",
    "# # This endpoint will be used in all examples in this quickstart.\n",
    "# ENDPOINT = \"\"\n",
    "\n",
    "# # Base url for the Verify and Facelist/Large Facelist operations\n",
    "# IMAGE_BASE_URL = 'https://csdx.blob.core.windows.net/resources/Face/Images/'\n",
    "\n",
    "# # Used in the Person Group Operations and Delete Person Group examples.\n",
    "# # You can call list_person_groups to print a list of preexisting PersonGroups.\n",
    "# # SOURCE_PERSON_GROUP_ID should be all lowercase and alphanumeric. For example, 'mygroupname' (dashes are OK).\n",
    "# PERSON_GROUP_ID = str(uuid.uuid4())  # assign a random ID (or name it anything)\n",
    "\n",
    "# # Used for the Delete Person Group example.\n",
    "# TARGET_PERSON_GROUP_ID = str(uuid.uuid4())  # assign a random ID (or name it anything)\n",
    "\n",
    "# '''\n",
    "# Authenticate\n",
    "# All examples use the same client.\n",
    "# '''\n",
    "\n",
    "# # Create an authenticated FaceClient.\n",
    "# face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))\n",
    "\n",
    "# '''\n",
    "# END - Authenticate\n",
    "# '''\n",
    "\n",
    "# '''Create/Train/Detect/Identify Person Group This example creates a Person Group, then trains it. It can then be used \n",
    "# to detect and identify faces in other group images. '''\n",
    "\n",
    "# print()\n",
    "# print('PERSON GROUP OPERATIONS')\n",
    "# print()\n",
    "\n",
    "# '''\n",
    "# Create the PersonGroup\n",
    "# '''\n",
    "\n",
    "# # Create empty Person Group. Person Group ID must be lower case, alphanumeric, and/or with '-', '_'.\n",
    "# print('Person group:', PERSON_GROUP_ID)\n",
    "# face_client.person_group.create(person_group_id=PERSON_GROUP_ID, name=PERSON_GROUP_ID)\n",
    "\n",
    "# # Define woman friend\n",
    "# woman = face_client.person_group_person.create(PERSON_GROUP_ID, \"Woman\")\n",
    "# # Define man friend\n",
    "# man = face_client.person_group_person.create(PERSON_GROUP_ID, \"Man\")\n",
    "# # Define child friend\n",
    "# child = face_client.person_group_person.create(PERSON_GROUP_ID, \"Child\")\n",
    "\n",
    "# '''\n",
    "# Detect faces and register to correct person\n",
    "# '''\n",
    "\n",
    "# # Find all jpeg images of friends in working directory\n",
    "# woman_images = [file for file in glob.glob('*.jpg') if file.startswith(\"w\")]\n",
    "# man_images = [file for file in glob.glob('*.jpg') if file.startswith(\"m\")]\n",
    "# child_images = [file for file in glob.glob('*.jpg') if file.startswith(\"ch\")]\n",
    "\n",
    "# # Add to a woman person\n",
    "# for image in woman_images:\n",
    "#     w = open(image, 'r+b')\n",
    "#     face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, woman.person_id, w)\n",
    "\n",
    "# # Add to a man person\n",
    "# for image in man_images:\n",
    "#     m = open(image, 'r+b')\n",
    "#     face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, man.person_id, m)\n",
    "\n",
    "# # Add to a child person\n",
    "# for image in child_images:\n",
    "#     ch = open(image, 'r+b')\n",
    "#     face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, child.person_id, ch)\n",
    "\n",
    "# '''\n",
    "# Train PersonGroup\n",
    "# '''\n",
    "# print()\n",
    "# print('Training the person group...')\n",
    "# # Train the person group\n",
    "# face_client.person_group.train(PERSON_GROUP_ID)\n",
    "\n",
    "# while True:\n",
    "#     training_status = face_client.person_group.get_training_status(PERSON_GROUP_ID)\n",
    "#     print(\"Training status: {}.\".format(training_status.status))\n",
    "#     print()\n",
    "#     if training_status.status is TrainingStatusType.succeeded:\n",
    "#         break\n",
    "#     elif training_status.status is TrainingStatusType.failed:\n",
    "#         sys.exit('Training the person group has failed.')\n",
    "#     time.sleep(5)\n",
    "\n",
    "# '''\n",
    "# Identify a face against a defined PersonGroup\n",
    "# '''\n",
    "# # Group image for testing against\n",
    "# test_image_array = glob.glob('test-image-person-group.jpg')\n",
    "# image = open(test_image_array[0], 'r+b')\n",
    "\n",
    "# print('Pausing for 60 seconds to avoid triggering rate limit on free account...')\n",
    "# time.sleep(60)\n",
    "\n",
    "# # Detect faces\n",
    "# face_ids = []\n",
    "# # We use detection model 3 to get better performance.\n",
    "# faces = face_client.face.detect_with_stream(image, detection_model='detection_03')\n",
    "# for face in faces:\n",
    "#     face_ids.append(face.face_id)\n",
    "\n",
    "# # Identify faces\n",
    "# results = face_client.face.identify(face_ids, PERSON_GROUP_ID)\n",
    "# print('Identifying faces in {}'.format(os.path.basename(image.name)))\n",
    "# if not results:\n",
    "#     print('No person identified in the person group for faces from {}.'.format(os.path.basename(image.name)))\n",
    "# for person in results:\n",
    "#     if len(person.candidates) > 0:\n",
    "#         print('Person for face ID {} is identified in {} with a confidence of {}.'.format(person.face_id, os.path.basename(image.name), person.candidates[0].confidence))  # Get topmost confidence score\n",
    "#     else:\n",
    "#         print('No person identified for face ID {} in {}.'.format(person.face_id, os.path.basename(image.name)))\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a3ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
